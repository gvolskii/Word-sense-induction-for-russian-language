{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9135dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pylab as plot\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from navec import Navec\n",
    "from os import path\n",
    "from pandas import read_csv\n",
    "from sklearn.cluster import AgglomerativeClustering, Birch\n",
    "from sklearn.metrics import adjusted_rand_score, silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ebeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/mosei/Downloads/navec_hudlit_v1_12B_500K_300d_100q.tar\"\n",
    "navec = Navec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b9365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сброс ограничений на количество выводимых рядов\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Сброс ограничений на число столбцов\n",
    "pd.set_option('display.max_columns', None)\n",
    " \n",
    "# Сброс ограничений на количество символов в записи\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caadf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gold_predict(df):\n",
    "    \"\"\" This method assigns the gold and predict fields to the data frame. \"\"\"\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df['gold'] = df['word'] + '_' + df['gold_sense_id']\n",
    "    df['predict'] = df['word'] + '_' + df['predict_sense_id']\n",
    "\n",
    "    return df\n",
    "\n",
    "def ari_per_word_weighted(df):\n",
    "    \"\"\" This method computes the ARI score weighted by the number of sentences per word. \"\"\"\n",
    "\n",
    "    df = gold_predict(df)\n",
    "\n",
    "    words = {word: (adjusted_rand_score(df_word.gold, df_word.predict), len(df_word))\n",
    "             for word in df.word.unique()\n",
    "             for df_word in (df.loc[df['word'] == word],)}\n",
    "\n",
    "    cumsum = sum(ari * count for ari, count in words.values())\n",
    "    total = sum(count for _, count in words.values())\n",
    "\n",
    "    assert total == len(df), 'please double-check the format of your data'\n",
    "\n",
    "    return cumsum / total, words\n",
    "\n",
    "def evaluate(dataset_fpath):\n",
    "    df = read_csv(dataset_fpath, sep='\\t', dtype={'gold_sense_id': str, 'predict_sense_id': str})\n",
    "    ari, words = ari_per_word_weighted(df)\n",
    "    vocab.append(words)\n",
    "    print('{}\\t{}\\t{}'.format('word', 'ari', 'count'))\n",
    "\n",
    "    for word in sorted(words.keys()):\n",
    "        print('{}\\t{:.6f}\\t{:d}'.format(word, *words[word]))\n",
    "\n",
    "    print('\\t{:.6f}\\t{:d}'.format(ari, len(df)))\n",
    "    return ari\n",
    "\n",
    "def save(df, corpus):\n",
    "    \"\"\"\n",
    "    :param df: Data Frame with predictions\n",
    "    :param corpus: dataset name\n",
    "    :return: path to the saved file\n",
    "    \"\"\"\n",
    "    output_fpath = corpus + \"_predictions.csv\"\n",
    "    df.to_csv(output_fpath, sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "    print(\"Generated dataset: {}\".format(output_fpath))\n",
    "    return output_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157f7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_BTS='C:/Users/mosei/OneDrive/Desktop/diploma/RUSSE_data/bts-rnc/'\n",
    "dataset = pd.read_csv(PATH_BTS +'train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f18e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mosei\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b19d8",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22eb6fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_vector(context):\n",
    "    \"\"\"\n",
    "    paramtres:\n",
    "        context -- list of words\n",
    "    output: \n",
    "        average vector of word embeddings of words in context\n",
    "    \"\"\"\n",
    "    vocab = set(context)\n",
    "    # length_con = len(context)\n",
    "    vectors = []\n",
    "    for word in vocab:\n",
    "        # tf = context.count(word) / length_con\n",
    "        if word in navec:\n",
    "            vectors.append(navec[word]) # np.linalg.norm((tf * idf[word]) * navec[word])\n",
    "    if not vectors:\n",
    "        vectors.append(navec['<pad>'])\n",
    "    vectors = np.array(vectors)\n",
    "    rows_num = np.shape(vectors)[0]\n",
    "    average_vector = np.sum(vectors, axis=0) / rows_num\n",
    "    return average_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41ccb7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_clusters(X_array):\n",
    "    clustering = [AgglomerativeClustering(n_clusters=i) for i in range(2, 9, 1)]\n",
    "    k = [2, 3, 4, 5, 6, 7, 8]\n",
    "    scores = [] \n",
    "    for j in range(7):\n",
    "        scores.append(silhouette_score(X_array, clustering[j].fit_predict(X_array)))\n",
    "    return k[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b284f0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут хочется взять некоторую окрестность слов вокруг целевого\n",
    "# Сохраню на память: bad_words = {'горн': 'горный', 'жаба': 'жабой', 'крыло': 'крыть', 'курица': 'кура', 'кура': 'курам'}\n",
    "def remove_target_word(context, positions):\n",
    "    positions = [i.split('-') for i in positions.split(',')]\n",
    "    for position in positions:\n",
    "        start = int(position[0])\n",
    "        end = int(position[1])\n",
    "        return context.replace(context[start:end+1], 'target')\n",
    "\n",
    "from natasha import (\n",
    "    MorphVocab, \n",
    "    Doc,\n",
    "    Segmenter,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsEmbedding\n",
    ")\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "\n",
    "def context_lemmatization(text, window_size, min_len): # (text, min_len, remove_word)\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:    \n",
    "        token.lemmatize(morph_vocab)\n",
    "    list_of_words = [word.lemma for word in doc.tokens if (word.pos != 'PUNCT' \n",
    "                                                  and len(word.lemma) > min_len\n",
    "                                                  and word.lemma not in stopwords.words('russian'))]\n",
    "    size_list = len(list_of_words)\n",
    "    if 'target' not in list_of_words:\n",
    "        print(list_of_words)\n",
    "    target_index = list_of_words.index('target')\n",
    "    inf = 0 if (target_index - window_size <= 0) else (target_index - window_size)\n",
    "    sup = size_list if (target_index + window_size >= size_list) else target_index + window_size\n",
    "    list_of_words = list_of_words[inf:sup]\n",
    "    list_of_words.remove('target')\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08e0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим целевое слово\n",
    "dataset['context'] = dataset.apply(lambda x: remove_target_word(x.context, x.positions), axis=1)\n",
    "# Убираем вообще все, что не буквы и не цифры\n",
    "dataset['context'] = dataset['context'].replace(\"\\W\", ' ', regex=True)\n",
    "# Цифры тоже убираем\n",
    "# dataset['context'] = dataset['context'].replace(\"\\d\", ' ', regex=True)\n",
    "# Количество пробелов подряд не больше одного\n",
    "dataset['context'] = dataset['context'].replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "dataset['context'] = dataset.apply(lambda x: context_lemmatization(x.context, min_len=1, window_size=5), axis=1)\n",
    "# (lambda x: context_lemmatization(x.context, 2, x.word), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b38176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Now analyzing балка ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 8\n",
      "Now analyzing вид ...\n",
      "Gold clusters: 3\n",
      "Predicted clusters: 2\n",
      "Now analyzing винт ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 4\n",
      "Now analyzing горн ...\n",
      "Gold clusters: 3\n",
      "Predicted clusters: 3\n",
      "Now analyzing губа ...\n",
      "Gold clusters: 3\n",
      "Predicted clusters: 5\n",
      "Now analyzing жаба ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 7\n",
      "Now analyzing клетка ...\n",
      "Gold clusters: 6\n",
      "Predicted clusters: 2\n",
      "Now analyzing крыло ...\n",
      "Gold clusters: 8\n",
      "Predicted clusters: 8\n",
      "Now analyzing купюра ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing курица ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 8\n",
      "Now analyzing лавка ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 4\n",
      "Now analyzing лайка ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing лев ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 2\n",
      "Now analyzing лира ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 7\n",
      "Now analyzing мина ...\n",
      "Gold clusters: 3\n",
      "Predicted clusters: 7\n",
      "Now analyzing мишень ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing обед ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 2\n",
      "Now analyzing оклад ...\n",
      "Gold clusters: 3\n",
      "Predicted clusters: 2\n",
      "Now analyzing опушка ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing полис ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing пост ...\n",
      "Gold clusters: 5\n",
      "Predicted clusters: 3\n",
      "Now analyzing поток ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing проказа ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 5\n",
      "Now analyzing пропасть ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 2\n",
      "Now analyzing проспект ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 7\n",
      "Now analyzing пытка ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing рысь ...\n",
      "Gold clusters: 2\n",
      "Predicted clusters: 2\n",
      "Now analyzing среда ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 2\n",
      "Now analyzing хвост ...\n",
      "Gold clusters: 4\n",
      "Predicted clusters: 7\n",
      "Now analyzing штамп ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset: C:/Users/mosei/OneDrive/Desktop/diploma/RUSSE_data/bts-rnc/_predictions.csv\n",
      "Generated dataset: C:/Users/mosei/OneDrive/Desktop/diploma/RUSSE_data/bts-rnc/_predictions.csv\n",
      "word\tari\tcount\n",
      "балка\t0.113004\t119\n",
      "вид\t0.192953\t77\n",
      "винт\t0.344404\t123\n",
      "горн\t0.222965\t51\n",
      "губа\t0.056219\t137\n",
      "жаба\t0.130896\t121\n",
      "клетка\t0.300575\t150\n",
      "крыло\t0.152255\t91\n",
      "купюра\t0.131664\t150\n",
      "курица\t0.085596\t93\n",
      "лавка\t0.139739\t149\n",
      "лайка\t0.352284\t99\n",
      "лев\t-0.018000\t44\n",
      "лира\t0.126502\t49\n",
      "мина\t0.027411\t65\n",
      "мишень\t0.346067\t121\n",
      "обед\t0.031373\t100\n",
      "оклад\t0.031408\t146\n",
      "опушка\t1.000000\t148\n",
      "полис\t-0.031379\t142\n",
      "пост\t0.354477\t144\n",
      "поток\t0.083191\t136\n",
      "проказа\t0.182760\t146\n",
      "пропасть\t0.050716\t127\n",
      "проспект\t0.031824\t139\n",
      "пытка\t-0.063925\t143\n",
      "рысь\t0.745667\t120\n",
      "среда\t0.213365\t144\n",
      "хвост\t0.085758\t121\n",
      "штамп\t0.405176\t96\n",
      "\t0.202950\t3491\n",
      "ARI: 0.20295026911222272\n",
      "Average number of senses: 3.2\n",
      "Variation of the number of senses: 1.4\n",
      "Minimum number of senses: 2\n",
      "Maximum number of senses: 8\n",
      "CPU times: total: 24.9 s\n",
      "Wall time: 4.96 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gold clusters: 4\n",
      "Predicted clusters: 2\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = []\n",
    "predicted = []\n",
    "goldsenses = []\n",
    "model_vector_size = 300\n",
    "testing = False\n",
    "pca = PCA(n_components=10)\n",
    "for word in dataset.word.unique():\n",
    "    print('Now analyzing', word, '...', file=sys.stderr)\n",
    "    subset = dataset[dataset.word == word]\n",
    "    goldsenses.append(len(subset.gold_sense_id.unique()))\n",
    "    contexts = []\n",
    "    matrix = np.empty((subset.shape[0], model_vector_size))\n",
    "    counter = 0\n",
    "    lengths = []\n",
    "    for line in subset.iterrows():\n",
    "        text = line[1].context\n",
    "        identifier = line[1].context_id\n",
    "        label = word + str(identifier)\n",
    "        contexts.append(label)\n",
    "        fp = average_vector(text)\n",
    "        lengths.append(len(text))\n",
    "        matrix[counter, :] = fp\n",
    "        counter += 1\n",
    "    matrix = pca.fit_transform(matrix)\n",
    "    matrix = matrix.copy(order='C')\n",
    "    clustering = Birch(n_clusters=number_of_clusters(matrix)).fit(matrix)\n",
    "    cur_predicted = clustering.labels_.tolist()\n",
    "    predicted += cur_predicted\n",
    "    gold = subset.gold_sense_id\n",
    "    print('Gold clusters:', len(set(gold)), file=sys.stderr)\n",
    "    print('Predicted clusters:', len(set(cur_predicted)), file=sys.stderr)     \n",
    "\n",
    "dataset.predict_sense_id = predicted\n",
    "fname = PATH_BTS\n",
    "save(dataset, fname)\n",
    "\n",
    "res = evaluate(save(dataset, fname))\n",
    "print('ARI:', res)\n",
    "print('Average number of senses:', np.average(goldsenses))\n",
    "print('Variation of the number of senses:', np.std(goldsenses))\n",
    "print('Minimum number of senses:', np.min(goldsenses))\n",
    "print('Maximum number of senses:', np.max(goldsenses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
